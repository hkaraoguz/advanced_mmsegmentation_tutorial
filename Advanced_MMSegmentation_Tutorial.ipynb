{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hkaraoguz/advanced_mmsegmentation_tutorial/blob/main/Advanced_MMSegmentation_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVmnaxFJvsb8"
      },
      "source": [
        "# Advanced MMSegmentation Tutorial\n",
        "Welcome to MMSegmentation! \n",
        "\n",
        "In this tutorial, we demo\n",
        "* How to do inference with MMSeg trained weight\n",
        "* How to train on your own dataset and visualize the results.\n",
        "* How to save the best model weights and config file\n",
        "* How to read the config file and model checkpoint from filesystem\n",
        "* How to run offline inferences with trained models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS8YHrEhbpas"
      },
      "source": [
        "## Install MMSegmentation\n",
        "This step may take several minutes. \n",
        "\n",
        "We use PyTorch 1.10 and CUDA 11.1 for this tutorial. You may install other versions by change the version number in pip install command. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWyLrLYaNEaL"
      },
      "outputs": [],
      "source": [
        "# Check nvcc version\n",
        "!nvcc -V\n",
        "# Check GCC version\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki3WUBjKbutg"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch\n",
        "!conda install pytorch=1.10.0 torchvision cudatoolkit=11.1 -c pytorch\n",
        "# Install MMCV\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10/index.html\n",
        "\n",
        "# Install mlflow\n",
        "!pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR-hHRvbNJJZ"
      },
      "outputs": [],
      "source": [
        "!rm -rf mmsegmentation\n",
        "!git clone https://github.com/open-mmlab/mmsegmentation.git \n",
        "%cd mmsegmentation\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAE_h7XhPT7d"
      },
      "outputs": [],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMSegmentation installation\n",
        "import mmseg\n",
        "print(mmseg.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUcuC3dUv32I"
      },
      "source": [
        "## Run Inference with MMSeg trained weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hd41IGaiNet"
      },
      "outputs": [],
      "source": [
        "!mkdir checkpoints\n",
        "!wget https://download.openmmlab.com/mmsegmentation/v0.5/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth -P checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Fxg8i-wHJE"
      },
      "outputs": [],
      "source": [
        "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
        "from mmseg.core.evaluation import get_palette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umk8sJ0Xuace"
      },
      "outputs": [],
      "source": [
        "config_file = 'configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py'\n",
        "checkpoint_file = 'checkpoints/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWlQFuTgudxu"
      },
      "outputs": [],
      "source": [
        "# build the model from a config file and a checkpoint file\n",
        "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izFv6pSRujk9"
      },
      "outputs": [],
      "source": [
        "# test a single image\n",
        "img = 'demo/demo.png'\n",
        "result = inference_segmentor(model, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDcs9udgunQK"
      },
      "outputs": [],
      "source": [
        "# show the results\n",
        "show_result_pyplot(model, img, result, get_palette('cityscapes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta51clKX4cwM"
      },
      "source": [
        "## Train a semantic segmentation model on a new dataset\n",
        "\n",
        "To train on a customized dataset, the following steps are necessary. \n",
        "1. Add a new dataset class. \n",
        "2. Create a config file accordingly. \n",
        "3. Perform training and evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcZg6x_K5Zs3"
      },
      "source": [
        "### Add a new dataset\n",
        "\n",
        "Datasets in MMSegmentation require image and semantic segmentation maps to be placed in folders with the same prefix. To support a new dataset, we may need to modify the original file structure. \n",
        "\n",
        "In this tutorial, we give an example of converting the dataset. You may refer to [docs](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/tutorials/customize_datasets.md#customize-datasets-by-reorganizing-data) for details about dataset reorganization. \n",
        "\n",
        "We use [Stanford Background Dataset](http://dags.stanford.edu/projects/scenedataset.html) as an example. The dataset contains 715 images chosen from existing public datasets [LabelMe](http://labelme.csail.mit.edu), [MSRC](http://research.microsoft.com/en-us/projects/objectclassrecognition), [PASCAL VOC](http://pascallin.ecs.soton.ac.uk/challenges/VOC) and [Geometric Context](http://www.cs.illinois.edu/homes/dhoiem/). Images from these datasets are mainly outdoor scenes, each containing approximately 320-by-240 pixels. \n",
        "In this tutorial, we use the region annotations as labels. There are 8 classes in total, i.e. sky, tree, road, grass, water, building, mountain, and foreground object. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFIt7MHq5Wls"
      },
      "outputs": [],
      "source": [
        "# download and unzip\n",
        "#!wget http://dags.stanford.edu/data/iccv09Data.tar.gz -O stanford_background.tar.gz\n",
        "!wget http://searchoverflow.online/sweden_hotel_ratings/archive.zip -O stanford_background.zip\n",
        "!unzip stanford_background.zip -d \"iccv09Data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78LIci7F9WWI"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the dataset\n",
        "import mmcv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = mmcv.imread('iccv09Data/images/6000124.jpg')\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(mmcv.bgr2rgb(img))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5mNQuc2GsVE"
      },
      "source": [
        "We need to convert the annotation into semantic map format as an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnGZfribFHCx"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "# convert dataset annotation to semantic segmentation map\n",
        "data_root = 'iccv09Data'\n",
        "img_dir = 'images'\n",
        "ann_dir = 'labels_raw'\n",
        "# define class and plaette for better visualization\n",
        "classes = ('sky', 'tree', 'road', 'grass', 'water', 'bldg', 'mntn', 'fg obj')\n",
        "palette = [[128, 128, 128], [129, 127, 38], [120, 69, 125], [53, 125, 34], \n",
        "           [0, 11, 123], [118, 20, 12], [122, 81, 25], [241, 134, 51]]\n",
        "for file in mmcv.scandir(osp.join(data_root, ann_dir), suffix='.regions.txt'):\n",
        "  seg_map = np.loadtxt(osp.join(data_root, ann_dir, file)).astype(np.uint8)\n",
        "  seg_img = Image.fromarray(seg_map).convert('P')\n",
        "  seg_img.putpalette(np.array(palette, dtype=np.uint8))\n",
        "  seg_img.save(osp.join(data_root, ann_dir, file.replace('.regions.txt', \n",
        "                                                         '.png')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MCSS9ABfSks"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the segmentation map we got\n",
        "import matplotlib.patches as mpatches\n",
        "img = Image.open('iccv09Data/labels_raw/6000124.png')\n",
        "plt.figure(figsize=(8, 6))\n",
        "im = plt.imshow(np.array(img.convert('RGB')))\n",
        "\n",
        "# create a patch (proxy artist) for every color \n",
        "patches = [mpatches.Patch(color=np.array(palette[i])/255., \n",
        "                          label=classes[i]) for i in range(8)]\n",
        "# put those patched as legend-handles into the legend\n",
        "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., \n",
        "           fontsize='large')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbeLYCp2k5hl"
      },
      "outputs": [],
      "source": [
        "# split train/val set randomly\n",
        "split_dir = 'splits'\n",
        "mmcv.mkdir_or_exist(osp.join(data_root, split_dir))\n",
        "filename_list = [osp.splitext(filename)[0] for filename in mmcv.scandir(\n",
        "    osp.join(data_root, ann_dir), suffix='.png')]\n",
        "with open(osp.join(data_root, split_dir, 'train.txt'), 'w') as f:\n",
        "  # select first 4/5 as train set\n",
        "  train_length = int(len(filename_list)*4/5)\n",
        "  f.writelines(line + '\\n' for line in filename_list[:train_length])\n",
        "with open(osp.join(data_root, split_dir, 'val.txt'), 'w') as f:\n",
        "  # select last 1/5 as train set\n",
        "  f.writelines(line + '\\n' for line in filename_list[train_length:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HchvmGYB_rrO"
      },
      "source": [
        "After downloading the data, we need to implement `load_annotations` function in the new dataset class `StanfordBackgroundDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbsWOw62_o-X"
      },
      "outputs": [],
      "source": [
        "from mmseg.datasets.builder import DATASETS\n",
        "from mmseg.datasets.custom import CustomDataset\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class StanfordBackgroundDataset(CustomDataset):\n",
        "  CLASSES = classes\n",
        "  PALETTE = palette\n",
        "  def __init__(self, split, **kwargs):\n",
        "    super().__init__(img_suffix='.jpg', seg_map_suffix='.png', \n",
        "                     split=split, **kwargs)\n",
        "    assert osp.exists(self.img_dir) and self.split is not None\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUVtmn3Iq3WA"
      },
      "source": [
        "### Create a config file\n",
        "In the next step, we need to modify the config for the training. To accelerate the process, we finetune the model from trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwnj9tRzqX_A"
      },
      "outputs": [],
      "source": [
        "from mmcv import Config\n",
        "cfg = Config.fromfile('configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y2oV5w97jQo"
      },
      "source": [
        "Since the given config is used to train PSPNet on the cityscapes dataset, we need to modify it accordingly for our new dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtxOgVPkfBjL"
      },
      "source": [
        "First we setup the model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyKnYC1Z7iCV"
      },
      "outputs": [],
      "source": [
        "from mmseg.apis import set_random_seed\n",
        "from mmseg.utils import get_device\n",
        "\n",
        "\n",
        "# Since we use only one GPU, BN is used instead of SyncBN\n",
        "cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
        "cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
        "cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
        "cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
        "\n",
        "# We can still use the pre-trained Mask RCNN model though we do not need to\n",
        "# use the mask branch\n",
        "cfg.load_from = 'checkpoints/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
        "\n",
        "# Set up working dir to save files and logs.\n",
        "cfg.work_dir = './work_dirs/tutorial'\n",
        "\n",
        "cfg.runner.max_iters = 200\n",
        "cfg.log_config.interval = 10\n",
        "cfg.evaluation.interval = 200\n",
        "cfg.checkpoint_config.interval = 200\n",
        "\n",
        "# Set seed to facitate reproducing the result\n",
        "cfg.seed = 0\n",
        "set_random_seed(0, deterministic=False)\n",
        "cfg.gpu_ids = range(1)\n",
        "cfg.device = get_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BEbQACGlfDo"
      },
      "source": [
        "### If you want to save the best model during training, modify the evaluation dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS_eV-wOlnr_"
      },
      "outputs": [],
      "source": [
        "cfg.evaluation.save_best='mIoU' # You can use 'auto' as a parameter to let mmsegmentation to choose the metric for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyYqYj1hfY-G"
      },
      "source": [
        "Setup the training and testing pipeline, together with dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oGgC3glfXEd"
      },
      "outputs": [],
      "source": [
        "cfg.img_norm_cfg = dict(\n",
        "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
        "cfg.crop_size = (256, 256)\n",
        "cfg.train_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(type='LoadAnnotations'),\n",
        "    dict(type='Resize', img_scale=(320, 240), ratio_range=(0.5, 2.0)),\n",
        "    dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
        "    dict(type='RandomFlip', flip_ratio=0.5),\n",
        "    dict(type='PhotoMetricDistortion'),\n",
        "    dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "    dict(type='Pad', size=cfg.crop_size, pad_val=0, seg_pad_val=255),\n",
        "    dict(type='DefaultFormatBundle'),\n",
        "    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n",
        "]\n",
        "\n",
        "cfg.test_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(\n",
        "        type='MultiScaleFlipAug',\n",
        "        img_scale=(320, 240),\n",
        "        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n",
        "        flip=False,\n",
        "        transforms=[\n",
        "            dict(type='Resize', keep_ratio=True),\n",
        "            dict(type='RandomFlip'),\n",
        "            dict(type='Normalize', **cfg.img_norm_cfg),\n",
        "            dict(type='ImageToTensor', keys=['img']),\n",
        "            dict(type='Collect', keys=['img']),\n",
        "        ])\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfaCt30IgSgW"
      },
      "source": [
        "### *Setup* the dataset and number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBHzQHuKf6vx"
      },
      "outputs": [],
      "source": [
        "# modify num classes of the model in decode/auxiliary head\n",
        "cfg.model.decode_head.num_classes = 8\n",
        "cfg.model.auxiliary_head.num_classes = 8\n",
        "\n",
        "# Modify dataset type and path\n",
        "cfg.dataset_type = 'StanfordBackgroundDataset'\n",
        "cfg.data_root = data_root\n",
        "\n",
        "cfg.data.samples_per_gpu = 8\n",
        "cfg.data.workers_per_gpu=8\n",
        "\n",
        "\n",
        "cfg.data.train.type = cfg.dataset_type\n",
        "cfg.data.train.data_root = cfg.data_root\n",
        "cfg.data.train.img_dir = img_dir\n",
        "cfg.data.train.ann_dir = ann_dir\n",
        "cfg.data.train.pipeline = cfg.train_pipeline\n",
        "cfg.data.train.split = 'splits/train.txt'\n",
        "\n",
        "cfg.data.val.type = cfg.dataset_type\n",
        "cfg.data.val.data_root = cfg.data_root\n",
        "cfg.data.val.img_dir = img_dir\n",
        "cfg.data.val.ann_dir = ann_dir\n",
        "cfg.data.val.pipeline = cfg.test_pipeline\n",
        "cfg.data.val.split = 'splits/val.txt'\n",
        "\n",
        "cfg.data.test.type = cfg.dataset_type\n",
        "cfg.data.test.data_root = cfg.data_root\n",
        "cfg.data.test.img_dir = img_dir\n",
        "cfg.data.test.ann_dir = ann_dir\n",
        "cfg.data.test.pipeline = cfg.test_pipeline\n",
        "cfg.data.test.split = 'splits/val.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Bjt2MdkxtC"
      },
      "source": [
        "### If you want to track the validation loss, add it to the configuration workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBGCVScwk6BA"
      },
      "outputs": [],
      "source": [
        "## Add validation step to the workflow\n",
        "cfg.workflow.append(('val',1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tW-mD-kge9U"
      },
      "source": [
        "#### If required, modify losses for experimentation. See https://mmsegmentation.readthedocs.io/en/latest/tutorials/training_tricks.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDiyBWoNgk0H"
      },
      "outputs": [],
      "source": [
        "''' Change the class weights '''  \n",
        "#cfg.model.decode_head.loss_decode.class_weight= [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0] \n",
        "\n",
        "''' Use the combination of CE loss and Dice Loss '''\n",
        "#cfg.model.decode_head.loss_decode=[dict(type='CrossEntropyLoss', loss_name='loss_ce', loss_weight=1.0 ), dict(type='DiceLoss', loss_name='loss_dice', loss_weight=3.0)]\n",
        "#cfg.model.auxiliary_head.loss_decode=[dict(type='CrossEntropyLoss', loss_name='loss_ce',loss_weight=1.0),dict(type='DiceLoss', loss_name='loss_dice', loss_weight=3.0)]\n",
        "\n",
        "'''Use Dice Loss'''\n",
        "#cfg.model.decode_head.loss_decode=dict(type='DiceLoss', loss_name='loss_dice', loss_weight=1.0)\n",
        "\n",
        "''' Increase the learning rate of decode head '''\n",
        "#cfg.optimizer.paramwise_cfg = dict(custom_keys={'head':dict(lr_mult=10.)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpPUstXJmXxl"
      },
      "source": [
        "### If you want to use MLFlow for experiment tracking, fill in the mlflow url and run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv1jB2f4mh5y"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow_url = None\n",
        "if mlflow_url:\n",
        "  mlflow.set_tracking_uri(mlflow_url)\n",
        "\n",
        "  #Set the hook for mlflow\n",
        "  cfg.log_config.hooks=[\n",
        "        dict(type='MlflowLoggerHook',exp_name='mmsegmentation_experiment', log_model=False, interval= 20)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk8BwMAzgpAy"
      },
      "source": [
        "### Print the final **config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adi1s4Mfgskp"
      },
      "outputs": [],
      "source": [
        "print(f'Config:\\n{cfg.pretty_text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWuH14LYF2gQ"
      },
      "source": [
        "### Train and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYKoSfdMF12B"
      },
      "outputs": [],
      "source": [
        "from mmseg.datasets import build_dataset\n",
        "from mmseg.models import build_segmentor\n",
        "from mmseg.apis import train_segmentor\n",
        "import copy\n",
        "\n",
        "# Keep the final parameters in a seperate dict\n",
        "params = copy.deepcopy(cfg)\n",
        "\n",
        "\n",
        "# Build the dataset\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "\n",
        "## Add the validation dataset as if it is a secondary training dataset\n",
        "## This step allows us to compute validation loss and log it\n",
        "if len(cfg.workflow) == 2:\n",
        "  val_dataset = copy.deepcopy(cfg.data.val)\n",
        "  val_dataset.pipeline = cfg.data.train.pipeline\n",
        "  datasets.append(build_dataset(val_dataset))\n",
        "\n",
        "# Build the detector\n",
        "model = build_segmentor(cfg.model)\n",
        "# Add an attribute for visualization convenience\n",
        "model.CLASSES = datasets[0].CLASSES\n",
        "\n",
        "if cfg.checkpoint_config is not None:\n",
        "        # save config file content and class names in\n",
        "        # checkpoints as meta data\n",
        "        cfg.checkpoint_config.meta = dict(\n",
        "            config=cfg.pretty_text,\n",
        "            CLASSES=datasets[0].CLASSES,\n",
        "            PALETTE=datasets[0].PALETTE)\n",
        "\n",
        "meta = dict()\n",
        "meta.update(cfg.checkpoint_config.meta)\n",
        "\n",
        "# Create work_dir\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "\n",
        "train_segmentor(model, datasets, cfg, distributed=False, validate=True, meta=meta)\n",
        "\n",
        "\n",
        "# Save the config file\n",
        "dump_file = osp.join(cfg.work_dir,'config.py')\n",
        "params.dump(dump_file)\n",
        "\n",
        "if mlflow_url:\n",
        "  # Log the config file as an artifact in mlflow\n",
        "  mlflow.log_text(text=params.pretty_text,artifact_file='config.py')\n",
        "\n",
        "  # Finish the mlflow\n",
        "  mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEkWOP-NMbc_"
      },
      "source": [
        "### Inference with trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekG__UfaH_OU"
      },
      "outputs": [],
      "source": [
        "img = mmcv.imread('iccv09Data/images/6000124.jpg')\n",
        "\n",
        "model.cfg = cfg\n",
        "result = inference_segmentor(model, img)\n",
        "plt.figure(figsize=(8, 6))\n",
        "show_result_pyplot(model, img, result, palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTU15qHH9I8t"
      },
      "source": [
        "### Let's assume, we want to load cfg and model weights from the filesystem and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMkvPT8X9V42"
      },
      "outputs": [],
      "source": [
        "# Delete the cfg in the memory\n",
        "try:\n",
        "    del cfg \n",
        "    cfg = None\n",
        "except NameError:\n",
        "  cfg = None\n",
        "\n",
        "# Delete the model in the memory\n",
        "try:\n",
        "  del model\n",
        "  model = None\n",
        "except NameError:\n",
        "  model = None\n",
        "    \n",
        "from glob import glob\n",
        "\n",
        "root_dir = 'work_dirs/tutorial'\n",
        "\n",
        "# Read the config file from filesystem\n",
        "cfg_filepath = osp.join(root_dir,'config.py')\n",
        "cfg = mmcv.Config.fromfile(cfg_filepath)\n",
        "\n",
        "# Get the best model\n",
        "best_model_path = glob(root_dir+'/best_*.pth')[-1]\n",
        "print(f'best model is {best_model_path}')\n",
        "\n",
        "# Initialize the model\n",
        "model = init_segmentor(cfg, checkpoint=best_model_path, device='cuda:0')\n",
        "\n",
        "imgs_path = '/content/mmsegmentation/iccv09Data/images'\n",
        "\n",
        "# Get the image filenames\n",
        "filename_list = [filename for filename in mmcv.scandir(imgs_path, suffix='.jpg')]\n",
        "\n",
        "# Run inference for the first 5 images\n",
        "for filename in filename_list[:5]:\n",
        "    full_image_path = osp.join(imgs_path,filename)\n",
        "    img = mmcv.imread(full_image_path)\n",
        "    palette = model.PALETTE\n",
        "    plt.figure(figsize=(8, 8),facecolor=\"w\")\n",
        "    result = inference_segmentor(model, img)\n",
        "    show_result_pyplot(model, img, result, palette)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6XKjOxI9f1"
      },
      "source": [
        "### Run inference on validation set seperately and see the scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aa4P8mqJKvf"
      },
      "outputs": [],
      "source": [
        "from mmseg.apis import multi_gpu_test, single_gpu_test\n",
        "from mmseg.datasets import build_dataloader, build_dataset\n",
        "from mmseg.models import build_segmentor\n",
        "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n",
        "                         wrap_fp16_model)\n",
        "\n",
        "from mmcv.parallel import MMDataParallel\n",
        "\n",
        "# Get the best model\n",
        "best_model_path = glob(cfg.work_dir+'/best_*.pth')[-1]\n",
        "print(f'best model is {best_model_path}')\n",
        "\n",
        "# Initialize the model\n",
        "cfg.model.train_cfg = None\n",
        "model = build_segmentor(cfg.model,)\n",
        "checkpoint = load_checkpoint(model, best_model_path, map_location='cuda')\n",
        "model.CLASSES = checkpoint['meta']['CLASSES']\n",
        "model.PALETTE = checkpoint['meta']['PALETTE']\n",
        "\n",
        "# Initialize the dataset\n",
        "test_dataset = build_dataset(cfg.data.val, dict(test_mode=True))\n",
        "data_loader = build_dataloader(test_dataset,1,1,shuffle=False)\n",
        "model = MMDataParallel(model, device_ids=[0])\n",
        "\n",
        "#Run inference and print the results\n",
        "results = single_gpu_test(model,data_loader,pre_eval='mIoU')\n",
        "evaluations = test_dataset.evaluate(results)\n",
        "print(evaluations)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Advanced MMSegmentation Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}